{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from data_analysis_config import col_name_map, TYPES_COLUMN_NAMES, categorize_by_quantile\n",
    "from itertools import chain\n",
    "import glob\n",
    "import os\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PRE-WORK**\n",
    "### PREPARE DATA\n",
    "1. Load data\n",
    "2. Rename columns\n",
    "3. Delete non-numeric columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe\n",
    "key_df = pd.read_csv('../csv_output/key_misspells.csv', sep=';')\n",
    "fp_df = pd.read_csv('../csv_output/fp.csv', sep=';')\n",
    "rand_df = pd.read_csv('../csv_output/rand_misspells.csv', sep=';')\n",
    "\n",
    "fp_df['amount_of_errors'] = 0\n",
    "\n",
    "# rename dataframe\n",
    "fp_df.rename(columns=col_name_map, inplace=True)\n",
    "key_df.rename(columns=col_name_map, inplace=True)\n",
    "rand_df.rename(columns=col_name_map, inplace=True)\n",
    "\n",
    "fp_df['Function'] = fp_df[\"Function\"].astype(\"category\")\n",
    "key_df['Function'] = key_df[\"Function\"].astype(\"category\")\n",
    "rand_df['Function'] = rand_df[\"Function\"].astype(\"category\")\n",
    "\n",
    "# delete strings\n",
    "fp_df = fp_df.select_dtypes(exclude=['object'])\n",
    "key_df = key_df.select_dtypes(exclude=['object'])\n",
    "rand_df = rand_df.select_dtypes(exclude=['object'])\n",
    "\n",
    "fp_df['Function'] = fp_df[\"Function\"].astype(\"object\")\n",
    "key_df['Function'] = key_df[\"Function\"].astype(\"object\")\n",
    "rand_df['Function'] = rand_df[\"Function\"].astype(\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Damerauâ€“Levenshtein normalized similarity'] + [col for cat in TYPES_COLUMN_NAMES.values() for col in cat] + ['Function', 'Amount of misspells (in input sentence)']\n",
    "fp_df = fp_df[columns]\n",
    "key_df = key_df[columns]\n",
    "rand_df = rand_df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add each sentence to a bucket: \n",
    "1. [1, 3) errors\n",
    "2. [3, 6) errors\n",
    "3. [6, +inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_df['Bucket'] = key_df['Amount of misspells (in input sentence)'].apply(lambda x: 1 if x < 3 else ( 2 if 3 <= x <= 5 else 3))\n",
    "rand_df['Bucket'] = key_df['Amount of misspells (in input sentence)'].apply(lambda x: 1 if x < 3 else ( 2 if 3 <= x <= 5 else 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_based_dataframes = [key_df.loc[key_df['Bucket'] == i].reset_index(drop=True) for i in range(1, 4)]\n",
    "rand_based_dataframe = [rand_df.loc[rand_df['Bucket'] == i].reset_index(drop=True)  for i in range(1, 4)]\n",
    "del key_df, rand_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define quantiles and categorize each of metrics (basing on perfect match & quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = pd.DataFrame()\n",
    "CATEGORIES = [1,2,3,4] # WHERE 1 IS TOP and 4 IS BOTTOM\n",
    "\n",
    "\n",
    "for by in list(chain.from_iterable(TYPES_COLUMN_NAMES.values())):\n",
    "    # define the quantiles\n",
    "    quantiles = {\"key\": [], 'rand': [], 'fp': []}\n",
    "    for i in range(len(key_based_dataframes)):\n",
    "        quantiles['key'].append(key_based_dataframes[i][by].quantile([0.25, 0.5, 0.75]).values)\n",
    "        quantiles['rand'].append(rand_based_dataframe[i][by].quantile([0.25, 0.5, 0.75]).values)\n",
    "    quantiles['fp'] = (fp_df[by].quantile([0.25, 0.5, 0.75]).values)\n",
    "\n",
    "\n",
    "    if 'normalized similarity'.lower() in by.lower():\n",
    "        ascending = True\n",
    "        goal = 1.0\n",
    "    else: \n",
    "        ascending = False\n",
    "        goal = 0.0\n",
    "\n",
    "\n",
    "    for i in range(len(quantiles['key'])):\n",
    "        config[f'{by} key group {i} bucket'] = np.concatenate((quantiles['key'][i], [goal, ascending]))\n",
    "        config[f'{by} rand group {i} bucket'] = np.concatenate(\n",
    "            (quantiles['rand'][i], [goal, ascending])\n",
    "        )\n",
    "    # assign to config quantiles, goal and ascending    \n",
    "    config[f'{by} fp quantiles'] = np.concatenate((quantiles['fp'], [goal, ascending]))\n",
    "\n",
    "    for k in range(len(key_based_dataframes)):\n",
    "        key_based_dataframes[k].insert(list(key_based_dataframes[k].columns).index(by), by + \" Result category\" , key_based_dataframes[k][by].apply(lambda x: categorize_by_quantile(x, goal, quantiles['key'][k], values = CATEGORIES, ascending_is_better=ascending)))\n",
    "        rand_based_dataframe[k].insert(list(rand_based_dataframe[k].columns).index(by), by + \" Result category\" , rand_based_dataframe[k][by].apply(lambda x: categorize_by_quantile(x, goal, quantiles['rand'][k], values =  CATEGORIES, ascending_is_better=ascending)))\n",
    "    fp_df.insert(list(fp_df.columns).index(by), by + \" Result category\" , fp_df[by].apply(lambda x: categorize_by_quantile(x, goal, quantiles['fp'], values =  CATEGORIES, ascending_is_better=ascending)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config.transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.columns\n",
    "names = {0: 'quantile 1', 1: 'quantile 2', 2: 'quantile 3', 3: 'Goal', 4: 'Ascending'}\n",
    "config.rename(names, inplace=True, axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define CONSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_FOR_RESULT_CATEGORY = [col for col in fp_df.columns if 'Result category' in col]\n",
    "COLUMN_NAMES_FOR_CATEGORIES = ['Full correctness', 'First range', 'Second range', 'Third range']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(key_based_dataframes)):\n",
    "    key_based_dataframes[k] = key_based_dataframes[k][COLUMNS_FOR_RESULT_CATEGORY + ['Function']]\n",
    "    rand_based_dataframe[k] = rand_based_dataframe[k][COLUMNS_FOR_RESULT_CATEGORY + ['Function']]\n",
    "fp_df = fp_df[COLUMNS_FOR_RESULT_CATEGORY + ['Function']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(data_frame, column_names_for_values: list, category_values_to_parse: list, normalized_values: bool = False):\n",
    "    if normalized_values:\n",
    "        column_names_for_values = [str(x) + ' [%]' for x in column_names_for_values]\n",
    "    scores = pd.DataFrame()\n",
    "    for by in data_frame.columns[:-1]:\n",
    "        # Compute the cross-tabulation of the 'Function' and the column\n",
    "        scores = pd.concat([scores, data_frame.groupby('Function')[by].agg(\n",
    "            [(f' {by} ({column_names_for_values[0].lower()})', lambda x: (x == category_values_to_parse[0]).sum()),\n",
    "             (f' {by} ({column_names_for_values[1].lower()})', lambda x: (x == category_values_to_parse[1]).sum()),\n",
    "             (f' {by} ({column_names_for_values[2].lower()})', lambda x: (x == category_values_to_parse[2]).sum()),\n",
    "             (f' {by} ({column_names_for_values[3].lower()})', lambda x: (x == category_values_to_parse[3]).sum()),\n",
    "\n",
    "             ])], axis=1)\n",
    "\n",
    "    scores = pd.concat([scores, data_frame.groupby('Function')[data_frame.columns[0]].agg(\n",
    "        [('Total count', 'count')])], axis=1)\n",
    "    if normalized_values:\n",
    "        for col_name in scores.columns[:-1]:\n",
    "            scores[col_name] = scores[col_name] / scores['Total count']\n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weitghted_average (dataframe, weights, only_average_columns=False):\n",
    "    for i in COLUMNS_FOR_RESULT_CATEGORY:\n",
    "        summary_column_names = [k for k in dataframe.columns if i in k]\n",
    "        dataframe[f'Weighted average {i}'] = dataframe[summary_column_names].dot(weights)\n",
    "    averages_columns = [k for k in dataframe.columns if \"Weighted average\" in k]\n",
    "\n",
    "    if only_average_columns:\n",
    "        return dataframe[averages_columns], averages_columns\n",
    "    return dataframe, averages_columns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {}\n",
    "QUANTIL_RANGE_WEIGTHS = {'fp': [1,0,0,0], 'key': [1,0,0,0], 'rand': [1,0,0,0]}\n",
    "# QUANTIL_RANGE_WEIGTHS = {'fp': [1,0,0,-0.5], 'key': [1,0,0,-0.5], 'rand': [1,0,0,-0.5]}\n",
    "                                                     # weigths for perfect match  & quantiles \n",
    "# fp \n",
    "summary_df, averages_columns = get_weitghted_average(get_summary(fp_df, column_names_for_values=COLUMN_NAMES_FOR_CATEGORIES, category_values_to_parse=CATEGORIES, normalized_values=True), QUANTIL_RANGE_WEIGTHS['fp'])\n",
    "summary_df['Summary'] = summary_df[averages_columns].sum(axis=1)\n",
    "summary['FP'] = summary_df\n",
    "for k in range(len(key_based_dataframes)):\n",
    "    # key based\n",
    "    summary_df, averages_columns = get_weitghted_average(get_summary(key_based_dataframes[k], column_names_for_values=COLUMN_NAMES_FOR_CATEGORIES, category_values_to_parse=CATEGORIES, normalized_values=True), QUANTIL_RANGE_WEIGTHS['key'])\n",
    "    summary_df['Summary'] = summary_df[averages_columns].sum(axis=1)\n",
    "    summary[f'key {k+1}'] = summary_df\n",
    "    # random\n",
    "    summary_df, averages_columns = get_weitghted_average(get_summary(rand_based_dataframe[k], column_names_for_values=COLUMN_NAMES_FOR_CATEGORIES, category_values_to_parse=CATEGORIES, normalized_values=True), QUANTIL_RANGE_WEIGTHS['rand'])\n",
    "    summary_df['Summary'] = summary_df[averages_columns].sum(axis=1)\n",
    "    summary[f'random {k+1}'] = summary_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the columns from each dataframe into a single dataframe\n",
    "merged_column_df = pd.concat([df[\"Summary\"] for df in summary.values()], axis=1,keys=summary.keys())\n",
    "\n",
    "# Set the index of the merged dataframe to the index of one of the source dataframes\n",
    "index_df = list(summary.values())[0]  # choose one of the source dataframes as index reference\n",
    "merged_column_df = merged_column_df.set_index(index_df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POINTS = [30, 25, 22, 18, 15] + list(range(10, 0, -1))\n",
    "POINTS = [i for i in range(5, 0, -1)]\n",
    "\n",
    "total = pd.DataFrame(0, index=merged_column_df.index, columns=merged_column_df.columns)\n",
    "for col in merged_column_df.columns:\n",
    "    sorted_df = merged_column_df[col].sort_values(ascending=False)\n",
    "    for i, tool in enumerate(sorted_df.index[:5]):\n",
    "        total.loc[tool, col] = POINTS[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR_TYPE_CATEGORY_WEIGTH = [1,1,1,1,1,1,1]\n",
    "# ERROR_TYPE_CATEGORY_WEIGTH = [0.8, 1, 1, 0.8, 0.8, 0.5, 0.5]\n",
    "total['TOTAL'] = np.dot(total, ERROR_TYPE_CATEGORY_WEIGTH)\n",
    "total.sort_values(by='TOTAL', inplace=True, ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "consts = {\n",
    "\"POINTS FOR TOP N\": tuple(POINTS) , \n",
    "\"ERROR_TYPE_CATEGORY_WEIGTH (FP, [KEY(n), RAND(n)] for n in range(3))\": tuple(ERROR_TYPE_CATEGORY_WEIGTH), \n",
    "\"PERFECT MATCH, 1-4 QUANTILES RESUULT WEIGHT\": tuple(QUANTIL_RANGE_WEIGTHS)\n",
    "}\n",
    "consts = pd.DataFrame.from_dict(consts, orient='index').transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"\".join(x for x in str(ERROR_TYPE_CATEGORY_WEIGTH) if (x.isalnum() or x in \"._\"))  + '_p_' + ''.join(x for x in str(POINTS) if (x.isalnum() or x in \"._-\")) + '_w_' + \"\".join(x for x in str(list(QUANTIL_RANGE_WEIGTHS.values())) if (x.isalnum() or x in \"._-\")).replace('.', '_') \n",
    "\n",
    "with pd.ExcelWriter(f'./excel_files/P2/{title}.xlsx') as writer:\n",
    "    consts.to_excel(writer, sheet_name='consts_config', index=False)\n",
    "    config.to_excel(writer, sheet_name='config', index=True)\n",
    "    for k in summary.keys():\n",
    "        summary[k].to_excel(writer, sheet_name=f'{k}', index=True)\n",
    "    merged_column_df.to_excel(writer, sheet_name='merged_columns', index=True)\n",
    "    total.to_excel(writer, sheet_name='total', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def check_occurences_in_excel_files(filepaths, sheet_name, row_counts):\n",
    "    files = [pd.read_excel(fp, sheet_name=sheet_name, index_col=0) for fp in filepaths]\n",
    "    file_combinations = list(combinations(range(len(filepaths)), 2))\n",
    "    \n",
    "    result = {\n",
    "        \"pairwise\": {f\"files_{i}_{j}\": {} for i, j in file_combinations},\n",
    "        \"all_files\": {},\n",
    "    }\n",
    "    \n",
    "    for row_count in row_counts:\n",
    "        # Pairwise comparison\n",
    "        for i, j in file_combinations:\n",
    "            common_indices = set(files[i].index[:row_count]).intersection(files[j].index[:row_count])\n",
    "            result[\"pairwise\"][f\"files_{i}_{j}\"][row_count] = common_indices\n",
    "        \n",
    "        # Comparison across all files\n",
    "        common_indices = set(files[0].index[:row_count])\n",
    "        for file in files[1:]:\n",
    "            common_indices = common_indices.intersection(file.index[:row_count])\n",
    "        result[\"all_files\"][row_count] = common_indices\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[121], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m row_counts \u001b[39m=\u001b[39m [\u001b[39m5\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m15\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m25\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(filepaths)\n\u001b[0;32m----> 6\u001b[0m comparison_results \u001b[39m=\u001b[39m check_occurences_in_excel_files(filepaths, sheet_name, row_counts)\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m comparison_results[\u001b[39m'\u001b[39m\u001b[39mall_files\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(i, \u001b[39mlen\u001b[39m(comparison_results[\u001b[39m'\u001b[39m\u001b[39mall_files\u001b[39m\u001b[39m'\u001b[39m][i]))\n",
      "Cell \u001b[0;32mIn[120], line 19\u001b[0m, in \u001b[0;36mcheck_occurences_in_excel_files\u001b[0;34m(filepaths, sheet_name, row_counts)\u001b[0m\n\u001b[1;32m     16\u001b[0m     result[\u001b[39m\"\u001b[39m\u001b[39mpairwise\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfiles_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mj\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m][row_count] \u001b[39m=\u001b[39m common_indices\n\u001b[1;32m     18\u001b[0m \u001b[39m# Comparison across all files\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m common_indices \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(files[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39mindex[:row_count])\n\u001b[1;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m files[\u001b[39m1\u001b[39m:]:\n\u001b[1;32m     21\u001b[0m     common_indices \u001b[39m=\u001b[39m common_indices\u001b[39m.\u001b[39mintersection(file\u001b[39m.\u001b[39mindex[:row_count])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "filepaths = glob.glob(os.path.join('./excel_files', '*.xlsx'))\n",
    "sheet_name = 'total'\n",
    "row_counts = [5, 10, 15, 20, 25]\n",
    "print(filepaths)\n",
    "\n",
    "comparison_results = check_occurences_in_excel_files(filepaths, sheet_name, row_counts)\n",
    "for i in comparison_results['all_files'].keys():\n",
    "    print(i, len(comparison_results['all_files'][i]))\n",
    "\n",
    "comparison_results['all_files'][20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def assign_values_for_occurences_in_excel_files(filepaths, sheet_name, row_counts):\n",
    "    files = [pd.read_excel(fp, sheet_name=sheet_name, index_col=0) for fp in filepaths]\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    for row_count in row_counts:\n",
    "        index_sums = defaultdict(int)\n",
    "\n",
    "        for file in files:\n",
    "            for idx, index_label in enumerate(file.index[:row_count]):\n",
    "                index_sums[index_label] += idx + 1  # Add 1 to account for 0-based indexing\n",
    "\n",
    "        result[row_count] = dict(index_sums)\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./excel_files/0.8110.80.80.50.5_p_302522181512151413121110987654321_w_100-0_5100-0_5100-0_5.xlsx', './excel_files/0.8110.80.80.50.5_p_151413121110987654321_w_100-0_5100-0_5100-0_5.xlsx', './excel_files/0.8110.80.80.50.5_p_151413121110987654321_w_100010001000.xlsx', './excel_files/0.8110.80.80.50.5_p_302522181512151413121110987654321_w_100010001000.xlsx']\n",
      "{'SpellChecker(Gingerit)': 4, 'SpellChecker(LanguageTool)': 5, 'SpellChecker(GramFormer)': 6, 'Autocorrect(GramFormer)': 8, 'GramFormer(Autocorrect)': 10, 'GramFormer(Gingerit)': 13, 'Gingerit(GramFormer)': 14}\n",
      "{'SpellChecker(Gingerit)': 4, 'Autocorrect(GramFormer)': 8, 'Autocorrect': 10, 'Gingerit': 10, 'LanguageTool(GramFormer)': 10, 'Gingerit(GramFormer)': 14, 'Autocorrect(SpellChecker)': 15, 'GramFormer(LanguageTool)': 16, 'Gingerit(SpellChecker)': 17, 'GramFormer(Gingerit)': 19, 'SpellChecker(GramFormer)': 21, 'SpellChecker(LanguageTool)': 23, 'GramFormer(Autocorrect)': 25, 'Gingerit(LanguageTool)': 28}\n",
      "{'SpellChecker(Gingerit)': 4, 'Autocorrect(GramFormer)': 8, 'Jamspell(Gingerit)': 12, 'Gingerit(GramFormer)': 14, 'Autocorrect(SpellChecker)': 15, 'GramFormer(Gingerit)': 19, 'SpellChecker(GramFormer)': 21, 'SpellChecker(LanguageTool)': 23, 'TextBlob(GramFormer)': 23, 'Autocorrect': 24, 'LanguageTool(GramFormer)': 24, 'GramFormer(Autocorrect)': 25, 'Autocorrect(Gingerit)': 26, 'GramFormer(SpellChecker)': 26, 'Autocorrect(TextBlob)': 27, 'GramFormer(LanguageTool)': 29, 'Gingerit(SpellChecker)': 31, 'Gingerit': 40, 'Gingerit(LanguageTool)': 42, 'Gingerit(Autocorrect)': 47}\n",
      "{'SpellChecker(Gingerit)': 4, 'Autocorrect(GramFormer)': 8, 'Gingerit(GramFormer)': 14, 'Autocorrect(Jamspell)': 18, 'GramFormer(Gingerit)': 19, 'Jamspell': 20, 'SpellChecker(GramFormer)': 21, 'SpellChecker(LanguageTool)': 23, 'TextBlob(GramFormer)': 23, 'Autocorrect': 24, 'LanguageTool(GramFormer)': 24, 'GramFormer(Autocorrect)': 25, 'Autocorrect(TextBlob)': 27, 'Jamspell(Gingerit)': 29, 'Gingerit(SpellChecker)': 31, 'Jamspell(Autocorrect)': 37, 'Gingerit(LanguageTool)': 42, 'GramFormer(SpellChecker)': 43, 'GramFormer(LanguageTool)': 45, 'Gingerit(Autocorrect)': 47, 'Autocorrect(SpellChecker)': 48, 'Gingerit': 58, 'Autocorrect(Gingerit)': 65, 'Gingerit(Jamspell)': 71, 'SpellChecker(Autocorrect)': 74}\n",
      "{'SpellChecker(Gingerit)': 4, 'Autocorrect(GramFormer)': 8, 'Gingerit(GramFormer)': 14, 'GramFormer(Gingerit)': 19, 'SpellChecker(GramFormer)': 21, 'SpellChecker(LanguageTool)': 23, 'LanguageTool(GramFormer)': 24, 'LanguageTool(SpellChecker)': 24, 'GramFormer(Autocorrect)': 25, 'SpellChecker': 25, 'Gingerit(LanguageTool)': 42, 'GramFormer(LanguageTool)': 45, 'Gingerit(Autocorrect)': 47, 'TextBlob(GramFormer)': 47, 'Autocorrect(SpellChecker)': 48, 'SpellChecker(Jamspell)': 49, 'Gingerit(SpellChecker)': 54, 'Gingerit': 58, 'GramFormer(SpellChecker)': 64, 'Autocorrect(Gingerit)': 65, 'Autocorrect(Jamspell)': 65, 'Autocorrect': 66, 'Gingerit(Jamspell)': 71, 'Jamspell(Gingerit)': 72, 'SpellChecker(Autocorrect)': 74, 'Autocorrect(TextBlob)': 77, 'Jamspell(Autocorrect)': 83, 'Jamspell': 86}\n"
     ]
    }
   ],
   "source": [
    "filepaths = glob.glob(os.path.join('./excel_files', '*.xlsx'))\n",
    "sheet_name = 'total'\n",
    "row_counts = [5, 10, 15, 20, 25]\n",
    "print(filepaths)\n",
    "\n",
    "comparison_results = assign_values_for_occurences_in_excel_files(filepaths, sheet_name, row_counts)\n",
    "# for i in comparison_results['all_files'].keys():\n",
    "#     print(i, len(comparison_results['all_files'][i]))\n",
    "\n",
    "# comparison_results['all_files'][25]\n",
    "for v in comparison_results.values():\n",
    "    print(dict(sorted(v.items(), key=lambda item: item[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(rc={'figure.figsize':(16,6)})\n",
    "# fig, axs = plt.subplots(ncols=3)\n",
    "# grouped_df = scores.groupby('bucket')\n",
    "# for i, col in enumerate(grouped_df.columns):\n",
    "#     sns.barplot(data=grouped_df, x=grouped_df.index, y=col, ax=axs[i], palette='viridis')\n",
    "#     axs[i].set_xlabel('Groups')\n",
    "#     axs[i].set_ylabel('Data')\n",
    "#     axs[i].set_title(col)\n",
    "\n",
    "\n",
    "# # scores[['Full correctness',\t'First range',\t'Second range']].plot(kind='bar', stacked = True)\n",
    "# # plt.set_xlabel('Function')\n",
    "# # plt.bar(scores.index, scores['Full correctness'], label='Full correctness')\n",
    "# # plt.bar(scores.index, scores['Full correctness'], bottom=scores['First range'], label='Second range')\n",
    "\n",
    "# sns.barplot(data=scores, x=scores.index, y=COLUMN_NAMES_FOR_CATEGORIES[0], color='blue', label=by)\n",
    "# sns.barplot(data=scores, x=scores.index, y=COLUMN_NAMES_FOR_CATEGORIES[1], color='green', bottom=scores[COLUMN_NAMES_FOR_CATEGORIES[0]], label=by)\n",
    "# sns.barplot(data=scores, x=scores.index, y=COLUMN_NAMES_FOR_CATEGORIES[2], color='red', bottom=scores[COLUMN_NAMES_FOR_CATEGORIES[0]] + scores[COLUMN_NAMES_FOR_CATEGORIES[1]], label=by)\n",
    "\n",
    "# # Set labels and legend\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.rcParams[\"axes.labelsize\"] = 7\n",
    "# plt.xlabel('Groups')\n",
    "# plt.ylabel('Data')\n",
    "# plt.title('Stacked Bar Plot')\n",
    "# plt.legend()\n",
    "\n",
    "# # Show plot\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # plt.bar =\n",
    "\n",
    "# ### potrojny "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
